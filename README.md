ðŸ“˜ Quantitative-BERT Model for Memo

This repository provides scripts and converted models for using ERNIE-3.0 Nano in memo classification tasks.
It includes both ONNX (PC client) and TFLite (mobile client) versions with fully built-in BERT support.

âœ¨ Overview

Base model: nghuyong/ernie-3.0-nano-zh

Conversion pipeline:

Hugging Face PyTorch â†’ ONNX (PC)

ONNX â†’ TensorFlow SavedModel

TensorFlow â†’ TFLite (mobile)

The converted models are used to classify user-written memos directly on client devices.

model_quint8_avx2.onnx: Quantized ONNX model optimized for desktop clients.

ernie3_nano_select.tflite: Quantized TFLite model optimized for mobile clients.

âš™ï¸ Environment Setup

We recommend Python 3.10 on Linux/CPU for reproducibility.

python -m pip install -U pip setuptools wheel
python -m pip install "transformers==4.41.1" "torch>=2.1,<3"
python -m pip install "onnx==1.14.0" "onnxruntime==1.15.1" "onnx-simplifier==0.4.33" "onnx-tf==1.10.0"
python -m pip install "tensorflow==2.13.0" "tensorflow-probability==0.21.0"
# Fix typing-extensions if TensorFlow downgrades it
python -m pip install --no-deps "typing-extensions==4.12.2"

ðŸ›  Conversion Pipeline
1. Export ONNX (with approximate GELU)

Script: export_onnx.py

Uses gelu_new to avoid Erf.

Fixes sequence length and sets opset_version=12.

2. Fix Gather indices â†’ int32

Script: onnx_fix_indices.py

Ensures Gather/GatherND use int32 indices.

3. Downgrade Unsqueeze ops

Script: downgrade_unsqueeze_to11.py

Converts Unsqueeze-13 â†’ Unsqueeze-11.

Sets global opset to 12.

4. Convert ONNX â†’ TensorFlow

Script: onnx_to_tf.py

Converts ONNX to TensorFlow SavedModel using onnx-tf.

5. Convert TensorFlow â†’ TFLite

Script: tf_to_tflite.py

Produces .tflite with only TFLITE_BUILTINS ops.

Optional optimizations: float16 / default quantization.

ðŸ“‚ Repository Structure
Quantitative-bert-model-for-memo/
â”‚
â”œâ”€â”€ model_quint8_avx2.onnx          # Quantized ONNX model (desktop)
â”œâ”€â”€ ernie3_nano_select.tflite       # Quantized TFLite model (mobile)
â”œâ”€â”€ sentencepiece.bpe.model         # Tokenizer model
â”œâ”€â”€ tokenizer.json                  # Hugging Face tokenizer config
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ vocab.txt
â”‚
â”œâ”€â”€ export_onnx.py                  # Step 1: PyTorch â†’ ONNX
â”œâ”€â”€ onnx_fix_indices.py             # Step 2: Fix indices
â”œâ”€â”€ downgrade_unsqueeze_to11.py     # Step 3: Downgrade ops
â”œâ”€â”€ onnx_to_tf.py                   # Step 4: ONNX â†’ TensorFlow
â”œâ”€â”€ tf_to_tflite.py                 # Step 5: TF â†’ TFLite
â”‚
â””â”€â”€ LICENSE

ðŸ“Œ Key Notes

No Erf ops â†’ safer for mobile deployment.

Only TFLITE_BUILTINS ops used (avoids Select TF Ops).

Fixed-length export ensures deterministic behavior.

Optimized for classification of short text memos on device.

ðŸ“œ License

Apache 2.0 License. See LICENSE
 for details.
